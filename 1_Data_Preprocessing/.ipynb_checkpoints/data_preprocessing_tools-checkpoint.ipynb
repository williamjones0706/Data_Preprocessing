{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick overview of Classes, Objects, and Methods\n",
    "Class: A class is the model of something we want to build. For example, if we make a house construction plan that gathers the instructions on how to build a house, then this construction plan is the class.\n",
    "\n",
    "Object: An object is an instance of the class. So if we take that same example of the house construction plan, then an object is simply a house. A house (the object) that was built by following the instructions of the construction plan (the class).\n",
    "And therefore there can be many objects of the same class, because we can build many houses from the construction plan.\n",
    "\n",
    "Method: A method is a tool we can use on the object to complete a specific action. So in this same example, a tool can be to open the main door of the house if a guest is coming. A method can also be seen as a function that is applied onto the object, takes some inputs (that were defined in the class) and returns some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will import the data set and setup matrix of features(features are synonomous with independet variables)\n",
    "# Features will usually be in the first columns to the left and the dependent variable vector on the far right\n",
    "# Create matrix of features seperate from the dependent variable vector containing only last column of data\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "\n",
    "# Take all the rows data and columns except the last column as a trick for any dataset for ML pre-processing (:-1 means all columns exluding last)\n",
    "X = dataset.iloc[:,:-1].values\n",
    "\n",
    "# Take the last column only and all its row data for the dependent variable vector\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# View the matrix of features\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# View the vector of features\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care of the missing data (nan values can cause errors in ML model)\n",
    "# You can simply delete the entire row or replace the missing value by the average of all the values in the column\n",
    "# Replacing with the average of the entire row is a way to maintain enough data for the model if getting rid\n",
    "# of the entire observation would reduce the dataset to a limited number of useful observations\n",
    "\n",
    "# The class SimpleImputer from sklearn allow performing replacement of the nan values (note: google collaboratory automatically suggests modules/packages when you start to type)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# you can choose to replace the values with the median, mean, mode, etc. but mean/avg is typically chosen\n",
    "# the var imputer is an object that will get built based on the class instructions when the matrix is applied\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Need the methods fit and transform in order to complete the process\n",
    "# Fit method connects imputer to matrix and looks for all missing values and computes average of column in the matrix but only looks at numerical values. \n",
    "# So we exclude the country string column (nornally include all numerial columns)\n",
    "\n",
    "imputer.fit(X[:,1:3])\n",
    "\n",
    "# But another method called transform is required to actually replace the missing values with the averages and return a new one\n",
    "# The x[:,1:3] = is required because if not the transform will only return the 2 columns of numerical data as the new matrix\n",
    "X[:,1:3] = imputer.transform(X[:,1:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# check the output for the new matrix of features\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data (aka creating numerical binary variables 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to be done because the ML model cannot use category strings to create numerical correlcations.\n",
    "# So strings have to be encoded or converted to numbers\n",
    "# The strings cannot be simply numbered in order, because the model might interpret the order as significant\n",
    "# For instance, if we simply label a column of something 0123456.... the model might think the first thing is more or less important\n",
    "# So instead we create columns for each string category value that is given a binary or dummy variable 0 or 1 for each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: In Google collaboratory, you get pop-up reminders to tell you exactly what the classes/methods do \n",
    "# and what their inputs are, it is similar to prompts someone would see in MS excel for formulas\n",
    "\n",
    "# The encoder in this one is only being applied to index 0 since this is just for the string features\n",
    "# If you do not apply the remainder = 'passthrough', you will only get a transformed output of the new binary columns \n",
    "# and lose the other columns\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])], remainder='passthrough')\n",
    "\n",
    "# this time we only need one line of code because for this package there is a method that combines the fit/transform function\n",
    "# The output of the transform method does not ouput a numpy array, so we must force this to happen \n",
    "# for the future model fit and training array requirement\n",
    "X = np.array(ct.fit_transform(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Check the output of the new array of features\n",
    "# We now see three new binary columns that encode a new column for the three countries in the original single column\n",
    "# Check the output to make sure that the binary columns represent the value correctly (in this case france was first and it is a 1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the label encoder class for this one to encode the dependent variable \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Since this is only one vector we do not need to enter anything in the encoder this time\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Since this is just a single vector it does not have to be a numpy array as the previous one\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Check the dependpent variable vector output\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split creates 4 sets( x train, x test, y train, y test)\n",
    "# The future ML model expects to get all of these sets as inputs which is why they must be created\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling\n",
    "Feature scaling must be applied AFTER the data is split into training and testing sets.\n",
    "Feature scaling consists of scaling all the feature/variables to ensure they all take values in the same scale in order to make sure one feature or another does not dominate in the model due to different scales (which it would treat similar to weighting)\n",
    "\n",
    "Now let me explain the why we have to apply feature scaling after splitting the data set into the training\n",
    "set and instead it's really obvious it is for the simple reason that the test set is supposed to be\n",
    "a brand new set on which you are going to evaluate your machine learning model.\n",
    "So it's exactly like you know you're training your machinery moral on your training set and then later\n",
    "on you know after it is trained you are going to deploy it on new observations.\n",
    "So what this means is that the test set is something you're not supposed to work with for the training\n",
    "and features killing is as you will see a technique that will get the mean and standard deviation of\n",
    "your feature.\n",
    "You know in order to perform the scaling.\n",
    "So if we apply feature scaling before the split then it will actually get the mean and standard deviation\n",
    "of all the values including the ones in the test set and since the test set is something you're not\n",
    "supposed to have you know like some future data in production.\n",
    "Well you know applying feature scaling on the original data before the split would cause some\n",
    "what we call information leakage on the test set.\n",
    "You know we would grab some information from the test set which we're not supposed to get because it\n",
    "is supposed to be new data with new observations.\n",
    "### So remember this the essential reason why you should not apply features scaling before the split is to prevent information leakage on the test set which you're not supposed to have until the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
